{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of Friday, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "outputId": "00926ca9-fe6f-4cb7-b3af-d6b0f218acec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here: In this activity, we will classify the movie reviews into three classes, those being positive, negative, or neutral. The dataset will be a sample text dataset of movie reviews. We will be gathering the following features that might help build the machine learning model. Term Frequency-Inverse Document Frequency (TFIDF) An important technique for this analysis TFIDF will help in determining the key terms of the reviews based on how often they appear relative to the prolonged archive. N-grams: This is capturing sentiments indicating phrases especially bigrams or trigrams which helps in contextual analysis. Sentiment Score: Each review can be processed with a TextBlob or similar library to get a sentiment breakdown if the review is positive, negative, or neutral. Word Count: This metric helps to know the number of excessive words in each review. Parts of Speech Tags: Counting parts of speech allows understanding of which sentiment-laden types of words adjectives, for example, are popular. Character Count: Counts the number of characters in a particular review and dares to hypothesize the depth and width of the review. Average Word Length: Related in a way to the review’s complexity metric, average word length in a review as expected may correlate with the complexity of a review. Exclamation Points: The number of exclamation marks can suggest the level of intensity and emotion.\\n'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here: In this activity, we will classify the movie reviews into three classes, those being positive, negative, or neutral. The dataset will be a sample text dataset of movie reviews. We will be gathering the following features that might help build the machine learning model. Term Frequency-Inverse Document Frequency (TFIDF) An important technique for this analysis TFIDF will help in determining the key terms of the reviews based on how often they appear relative to the prolonged archive. N-grams: This is capturing sentiments indicating phrases especially bigrams or trigrams which helps in contextual analysis. Sentiment Score: Each review can be processed with a TextBlob or similar library to get a sentiment breakdown if the review is positive, negative, or neutral. Word Count: This metric helps to know the number of excessive words in each review. Parts of Speech Tags: Counting parts of speech allows understanding of which sentiment-laden types of words adjectives, for example, are popular. Character Count: Counts the number of characters in a particular review and dares to hypothesize the depth and width of the review. Average Word Length: Related in a way to the review’s complexity metric, average word length in a review as expected may correlate with the complexity of a review. Exclamation Points: The number of exclamation marks can suggest the level of intensity and emotion.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "outputId": "2ea753be-5e45-4c9c-dff9-24513224d306"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample movie reviews data\n",
            "                                               review sentiment\n",
            "0        I loved this movie! The actors were amazing  positive\n",
            "1       This was the worst purchase I have ever made  negative\n",
            "2  Its okay, the movie did not reach my expectations   neutral\n",
            "3  Highly recommend! Amazing screenplay, directio...  positive\n",
            "4     Dont watch this movie. Its a total time waste!  negative\n",
            "\n",
            " Extracted TF-IDF Features are\n",
            "      acting    actors   amazing      bad       did  direction      dont  \\\n",
            "0  0.000000  0.480631  0.480631  0.00000  0.000000   0.000000  0.000000   \n",
            "1  0.000000  0.000000  0.000000  0.00000  0.000000   0.000000  0.000000   \n",
            "2  0.000000  0.000000  0.000000  0.00000  0.439955   0.000000  0.000000   \n",
            "3  0.414196  0.308050  0.308050  0.00000  0.000000   0.414196  0.000000   \n",
            "4  0.000000  0.000000  0.000000  0.00000  0.000000   0.000000  0.415853   \n",
            "5  0.000000  0.000000  0.429504  0.00000  0.000000   0.000000  0.000000   \n",
            "6  0.000000  0.000000  0.000000  0.00000  0.000000   0.000000  0.000000   \n",
            "7  0.000000  0.000000  0.000000  0.57631  0.000000   0.000000  0.000000   \n",
            "8  0.000000  0.000000  0.000000  0.00000  0.000000   0.000000  0.514170   \n",
            "9  0.000000  0.286645  0.000000  0.00000  0.327639   0.000000  0.000000   \n",
            "\n",
            "   expectations      flop     half  ...     skip     story  superb      time  \\\n",
            "0      0.000000  0.000000  0.00000  ...  0.00000  0.000000  0.0000  0.000000   \n",
            "1      0.000000  0.000000  0.00000  ...  0.00000  0.000000  0.0000  0.000000   \n",
            "2      0.517538  0.000000  0.00000  ...  0.00000  0.000000  0.0000  0.000000   \n",
            "3      0.000000  0.000000  0.00000  ...  0.00000  0.000000  0.0000  0.000000   \n",
            "4      0.000000  0.000000  0.00000  ...  0.00000  0.000000  0.0000  0.415853   \n",
            "5      0.000000  0.000000  0.00000  ...  0.00000  0.490928  0.5775  0.000000   \n",
            "6      0.000000  0.000000  0.00000  ...  0.00000  0.000000  0.0000  0.403194   \n",
            "7      0.000000  0.000000  0.57631  ...  0.00000  0.000000  0.0000  0.000000   \n",
            "8      0.000000  0.000000  0.00000  ...  0.60484  0.000000  0.0000  0.000000   \n",
            "9      0.000000  0.385416  0.00000  ...  0.00000  0.327639  0.0000  0.000000   \n",
            "\n",
            "      total  valuable     waste    wasted     watch     worst  \n",
            "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.707107  \n",
            "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "4  0.415853  0.000000  0.489186  0.000000  0.415853  0.000000  \n",
            "5  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "6  0.000000  0.474295  0.000000  0.474295  0.000000  0.000000  \n",
            "7  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "8  0.000000  0.000000  0.000000  0.000000  0.514170  0.000000  \n",
            "9  0.327639  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[10 rows x 32 columns]\n",
            "\n",
            " Extracted N-grams Features are\n",
            "    acting actors  actors amazing  actors did  amazing screenplay  \\\n",
            "0       0.000000         0.57735    0.000000            0.000000   \n",
            "1       0.000000         0.00000    0.000000            0.000000   \n",
            "2       0.000000         0.00000    0.000000            0.000000   \n",
            "3       0.408248         0.00000    0.000000            0.408248   \n",
            "4       0.000000         0.00000    0.000000            0.000000   \n",
            "5       0.000000         0.00000    0.000000            0.000000   \n",
            "6       0.000000         0.00000    0.000000            0.000000   \n",
            "7       0.000000         0.00000    0.000000            0.000000   \n",
            "8       0.000000         0.00000    0.000000            0.000000   \n",
            "9       0.000000         0.00000    0.359846            0.000000   \n",
            "\n",
            "   amazing story  bad movie  did justice  did reach  direction acting  \\\n",
            "0        0.00000    0.00000     0.000000        0.0          0.000000   \n",
            "1        0.00000    0.00000     0.000000        0.0          0.000000   \n",
            "2        0.00000    0.00000     0.000000        0.5          0.000000   \n",
            "3        0.00000    0.00000     0.000000        0.0          0.408248   \n",
            "4        0.00000    0.00000     0.000000        0.0          0.000000   \n",
            "5        0.57735    0.00000     0.000000        0.0          0.000000   \n",
            "6        0.00000    0.00000     0.000000        0.0          0.000000   \n",
            "7        0.00000    0.57735     0.000000        0.0          0.000000   \n",
            "8        0.00000    0.00000     0.000000        0.0          0.000000   \n",
            "9        0.00000    0.00000     0.359846        0.0          0.000000   \n",
            "\n",
            "   dont skip  ...  skip movie  story screenplay  superb amazing  time waste  \\\n",
            "0    0.00000  ...     0.00000          0.000000         0.00000    0.000000   \n",
            "1    0.00000  ...     0.00000          0.000000         0.00000    0.000000   \n",
            "2    0.00000  ...     0.00000          0.000000         0.00000    0.000000   \n",
            "3    0.00000  ...     0.00000          0.000000         0.00000    0.000000   \n",
            "4    0.00000  ...     0.00000          0.000000         0.00000    0.460158   \n",
            "5    0.00000  ...     0.00000          0.000000         0.57735    0.000000   \n",
            "6    0.00000  ...     0.00000          0.000000         0.00000    0.000000   \n",
            "7    0.00000  ...     0.00000          0.000000         0.00000    0.000000   \n",
            "8    0.57735  ...     0.57735          0.000000         0.00000    0.000000   \n",
            "9    0.00000  ...     0.00000          0.359846         0.00000    0.000000   \n",
            "\n",
            "   total flop  total time  valuable time  wasted hours  watch movie  \\\n",
            "0    0.000000    0.000000            0.0           0.0     0.000000   \n",
            "1    0.000000    0.000000            0.0           0.0     0.000000   \n",
            "2    0.000000    0.000000            0.0           0.0     0.000000   \n",
            "3    0.000000    0.000000            0.0           0.0     0.000000   \n",
            "4    0.000000    0.460158            0.0           0.0     0.460158   \n",
            "5    0.000000    0.000000            0.0           0.0     0.000000   \n",
            "6    0.000000    0.000000            0.5           0.5     0.000000   \n",
            "7    0.000000    0.000000            0.0           0.0     0.000000   \n",
            "8    0.000000    0.000000            0.0           0.0     0.000000   \n",
            "9    0.359846    0.000000            0.0           0.0     0.000000   \n",
            "\n",
            "   worst purchase  \n",
            "0             0.0  \n",
            "1             1.0  \n",
            "2             0.0  \n",
            "3             0.0  \n",
            "4             0.0  \n",
            "5             0.0  \n",
            "6             0.0  \n",
            "7             0.0  \n",
            "8             0.0  \n",
            "9             0.0  \n",
            "\n",
            "[10 rows x 39 columns]\n",
            "\n",
            " Extracted Sentiment scores are\n",
            "    Sentiment score is\n",
            "0            0.737500\n",
            "1           -1.000000\n",
            "2            0.500000\n",
            "3            0.266667\n",
            "4           -0.125000\n",
            "5            0.800000\n",
            "6           -0.250000\n",
            "7            0.233333\n",
            "8            0.000000\n",
            "9            0.000000\n",
            "\n",
            " Extracted pas tags are\n",
            " [[('I', 'PRP'), ('loved', 'VBD'), ('this', 'DT'), ('movie', 'NN'), ('!', '.'), ('The', 'DT'), ('actors', 'NNS'), ('were', 'VBD'), ('amazing', 'VBG')], [('This', 'DT'), ('was', 'VBD'), ('the', 'DT'), ('worst', 'JJS'), ('purchase', 'NN'), ('I', 'PRP'), ('have', 'VBP'), ('ever', 'RB'), ('made', 'VBN')], [('Its', 'PRP$'), ('okay', 'NN'), (',', ','), ('the', 'DT'), ('movie', 'NN'), ('did', 'VBD'), ('not', 'RB'), ('reach', 'VB'), ('my', 'PRP$'), ('expectations', 'NNS')], [('Highly', 'NNP'), ('recommend', 'NN'), ('!', '.'), ('Amazing', 'NNP'), ('screenplay', 'NN'), (',', ','), ('direction', 'NN'), ('and', 'CC'), ('acting', 'NN'), ('by', 'IN'), ('the', 'DT'), ('actors', 'NNS')], [('Dont', 'NNP'), ('watch', 'NN'), ('this', 'DT'), ('movie', 'NN'), ('.', '.'), ('Its', 'PRP$'), ('a', 'DT'), ('total', 'JJ'), ('time', 'NN'), ('waste', 'NN'), ('!', '.')], [('Just', 'RB'), ('Superb', 'NNP'), ('.', '.'), ('Amazing', 'NNP'), ('Story', 'NN')], [('Just', 'RB'), ('wasted', 'VBN'), ('three', 'CD'), ('hours', 'NNS'), ('of', 'IN'), ('my', 'PRP$'), ('valuable', 'JJ'), ('time', 'NN'), ('!', '.')], [('Not', 'RB'), ('bad', 'JJ'), (',', ','), ('the', 'DT'), ('movie', 'NN'), ('was', 'VBD'), ('okay', 'VBN'), ('for', 'IN'), ('the', 'DT'), ('first', 'JJ'), ('half', 'NN')], [('Dont', 'NNP'), ('skip', 'NN'), ('this', 'DT'), ('movie', 'NN'), ('!', '.'), ('Everyone', 'NN'), ('must', 'MD'), ('watch', 'VB'), ('it', 'PRP'), ('.', '.')], [('The', 'DT'), ('movie', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('total', 'JJ'), ('flop', 'NN'), ('.', '.'), ('No', 'NNP'), ('story', 'NN'), (',', ','), ('no', 'DT'), ('screenplay', 'NN'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('actors', 'NNS'), ('did', 'VBD'), ('not', 'RB'), ('do', 'VB'), ('justice', 'NN'), ('for', 'IN'), ('their', 'PRP$'), ('roles', 'NNS')]]\n",
            "\n",
            " Word counts are\n",
            "    Word Count\n",
            "0           8\n",
            "1           9\n",
            "2           9\n",
            "3          10\n",
            "4           9\n",
            "5           4\n",
            "6           8\n",
            "7          10\n",
            "8           8\n",
            "9          20\n",
            "\n",
            " Exclamation marks count is\n",
            "    Exclamation Mark Count\n",
            "0                       1\n",
            "1                       0\n",
            "2                       0\n",
            "3                       1\n",
            "4                       1\n",
            "5                       0\n",
            "6                       1\n",
            "7                       0\n",
            "8                       1\n",
            "9                       0\n",
            "\n",
            " Dataset after combining all features\n",
            "      acting    actors   amazing      bad       did  direction      dont  \\\n",
            "0  0.000000  0.480631  0.480631  0.00000  0.000000   0.000000  0.000000   \n",
            "1  0.000000  0.000000  0.000000  0.00000  0.000000   0.000000  0.000000   \n",
            "2  0.000000  0.000000  0.000000  0.00000  0.439955   0.000000  0.000000   \n",
            "3  0.414196  0.308050  0.308050  0.00000  0.000000   0.414196  0.000000   \n",
            "4  0.000000  0.000000  0.000000  0.00000  0.000000   0.000000  0.415853   \n",
            "5  0.000000  0.000000  0.429504  0.00000  0.000000   0.000000  0.000000   \n",
            "6  0.000000  0.000000  0.000000  0.00000  0.000000   0.000000  0.000000   \n",
            "7  0.000000  0.000000  0.000000  0.57631  0.000000   0.000000  0.000000   \n",
            "8  0.000000  0.000000  0.000000  0.00000  0.000000   0.000000  0.514170   \n",
            "9  0.000000  0.286645  0.000000  0.00000  0.327639   0.000000  0.000000   \n",
            "\n",
            "   expectations      flop     half  ...  time waste  total flop  total time  \\\n",
            "0      0.000000  0.000000  0.00000  ...    0.000000    0.000000    0.000000   \n",
            "1      0.000000  0.000000  0.00000  ...    0.000000    0.000000    0.000000   \n",
            "2      0.517538  0.000000  0.00000  ...    0.000000    0.000000    0.000000   \n",
            "3      0.000000  0.000000  0.00000  ...    0.000000    0.000000    0.000000   \n",
            "4      0.000000  0.000000  0.00000  ...    0.460158    0.000000    0.460158   \n",
            "5      0.000000  0.000000  0.00000  ...    0.000000    0.000000    0.000000   \n",
            "6      0.000000  0.000000  0.00000  ...    0.000000    0.000000    0.000000   \n",
            "7      0.000000  0.000000  0.57631  ...    0.000000    0.000000    0.000000   \n",
            "8      0.000000  0.000000  0.00000  ...    0.000000    0.000000    0.000000   \n",
            "9      0.000000  0.385416  0.00000  ...    0.000000    0.359846    0.000000   \n",
            "\n",
            "   valuable time  wasted hours  watch movie  worst purchase  \\\n",
            "0            0.0           0.0     0.000000             0.0   \n",
            "1            0.0           0.0     0.000000             1.0   \n",
            "2            0.0           0.0     0.000000             0.0   \n",
            "3            0.0           0.0     0.000000             0.0   \n",
            "4            0.0           0.0     0.460158             0.0   \n",
            "5            0.0           0.0     0.000000             0.0   \n",
            "6            0.5           0.5     0.000000             0.0   \n",
            "7            0.0           0.0     0.000000             0.0   \n",
            "8            0.0           0.0     0.000000             0.0   \n",
            "9            0.0           0.0     0.000000             0.0   \n",
            "\n",
            "   Sentiment score is  Word Count  Exclamation Mark Count  \n",
            "0            0.737500           8                       1  \n",
            "1           -1.000000           9                       0  \n",
            "2            0.500000           9                       0  \n",
            "3            0.266667          10                       1  \n",
            "4           -0.125000           9                       1  \n",
            "5            0.800000           4                       0  \n",
            "6           -0.250000           8                       1  \n",
            "7            0.233333          10                       0  \n",
            "8            0.000000           8                       1  \n",
            "9            0.000000          20                       0  \n",
            "\n",
            "[10 rows x 74 columns]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/venya/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/venya/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "#Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "#Downloading 'punkt' for tokenization\n",
        "nltk.download('punkt')\n",
        "#Downloading 'averaged_perceptron_tagger' for part-of-speech tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#Sample movie reviews dataset in text format\n",
        "data = {\n",
        "    'review':[\n",
        "        \"I loved this movie! The actors were amazing\",\n",
        "        \"This was the worst purchase I have ever made\",\n",
        "        \"Its okay, the movie did not reach my expectations\",\n",
        "        \"Highly recommend! Amazing screenplay, direction and acting by the actors\",\n",
        "        \"Dont watch this movie. Its a total time waste!\",\n",
        "        \"Just Superb. Amazing Story\",\n",
        "        \"Just wasted three hours of my valuable time!\",\n",
        "        \"Not bad, the movie was okay for the first half\",\n",
        "        \"Dont skip this movie! Everyone must watch it.\",\n",
        "        \"The movie is a total flop. No story, no screenplay, and the actors did not do justice for their roles\",\n",
        "\n",
        "    ],\n",
        "    'sentiment':[\n",
        "        'positive',\n",
        "        'negative',\n",
        "        'neutral',\n",
        "        'positive',\n",
        "        'negative',\n",
        "        'positive',\n",
        "        'negative',\n",
        "        'neutral',\n",
        "        'positive',\n",
        "        'negative'\n",
        "    ]\n",
        "}\n",
        "#Creating the DataFrame to extract the features\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "#Display the first 5 rows of dataset\n",
        "print(\"Sample movie reviews data\\n\", df.head())\n",
        "\n",
        "#Extracting TF-IDF Features\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "#Fitting and transforming the reviews to TF-IDF representation\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['review'])\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"\\n Extracted TF-IDF Features are\\n\",tfidf_df)\n",
        "\n",
        "#Extracting N-grams\n",
        "ngram_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(2,2))\n",
        "\n",
        "#Fitting and transforming the reviews to TF-IDF representation\n",
        "X_ngrams = ngram_vectorizer.fit_transform(df['review'])\n",
        "ngram_df = pd.DataFrame(X_ngrams.toarray(), columns=ngram_vectorizer.get_feature_names_out())\n",
        "print(\"\\n Extracted N-grams Features are\\n\",ngram_df)\n",
        "\n",
        "#Calculating sentiment score\n",
        "def sentiment(review):\n",
        "    blob = TextBlob(review)\n",
        "    #Returns sentiment score between -1 and 1. -1 indicates negative, 1 indicates positive\n",
        "    return blob.sentiment.polarity\n",
        "\n",
        "#Calculating sentiment scores for each review\n",
        "sentiment_score = [sentiment(review) for review in df['review']]\n",
        "sentiment_df = pd.DataFrame({'Sentiment score is': sentiment_score})\n",
        "print(\"\\n Extracted Sentiment scores are\\n\",sentiment_df)\n",
        "\n",
        "#Defining function to extract Part_of_Speech tags\n",
        "\n",
        "def pos_tags(review):\n",
        "    #Tokenizing the review text\n",
        "    tokens = nltk.word_tokenize(review)\n",
        "    tag = pos_tag(tokens)\n",
        "    return tag\n",
        "\n",
        "#Generating pos tags for all reviews\n",
        "pos_list = [pos_tags(review) for review in df['review']]\n",
        "print(\"\\n Extracted pas tags are\\n\",pos_list)\n",
        "\n",
        "#Calculating word count\n",
        "#Counting the number of words in each review\n",
        "word_count = [len(review.split()) for review in df['review']]\n",
        "wordcount_df = pd.DataFrame({'Word Count' : word_count})\n",
        "print(\"\\n Word counts are\\n\",wordcount_df)\n",
        "\n",
        "#Calculating the exclamation Marks\n",
        "exclamation_count = [review.count('!') for review in df['review']]\n",
        "exclamationcount_df = pd.DataFrame({'Exclamation Mark Count' : exclamation_count})\n",
        "print(\"\\n Exclamation marks count is\\n\",exclamationcount_df)\n",
        "\n",
        "#Final dataset after combining all features\n",
        "final_df = pd.concat([tfidf_df,ngram_df,sentiment_df,wordcount_df,exclamationcount_df ],axis = 1)\n",
        "print(\"\\n Dataset after combining all features\\n\",final_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CRuXfV570ng",
        "outputId": "20bbda05-a75c-4b3a-b0a0-0e874179e432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chi square results\n",
            "          Feature  Chi_square value\n",
            "16          okay          3.719483\n",
            "9           half          2.305240\n",
            "3            bad          2.305240\n",
            "18         reach          2.070152\n",
            "7   expectations          2.070152\n",
            "2        amazing          1.827277\n",
            "25          time          1.228571\n",
            "26         total          1.115237\n",
            "17      purchase          1.060660\n",
            "31         worst          1.060660\n",
            "14         loved          0.969367\n",
            "22          skip          0.907261\n",
            "24        superb          0.866250\n",
            "4            did          0.842853\n",
            "28         waste          0.733779\n",
            "11         hours          0.711443\n",
            "29        wasted          0.711443\n",
            "27      valuable          0.711443\n",
            "10        highly          0.621294\n",
            "5      direction          0.621294\n",
            "0         acting          0.621294\n",
            "19     recommend          0.621294\n",
            "20         roles          0.578124\n",
            "8           flop          0.578124\n",
            "13       justice          0.578124\n",
            "1         actors          0.561812\n",
            "6           dont          0.245498\n",
            "30         watch          0.245498\n",
            "23         story          0.245358\n",
            "15         movie          0.240518\n",
            "12          just          0.234291\n",
            "21    screenplay          0.171037\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "#Preparing labels for feature selection\n",
        "label_Encoder = LabelEncoder()\n",
        "\n",
        "#Transforming categorical sentiment labels into numerical\n",
        "encoded_labels = label_encoder.fit_transform(df['sentiment'])\n",
        "\n",
        "#Applying Chi-square test for tfidf features\n",
        "chi_square_score, _ = chi2(X_tfidf,encoded_labels)\n",
        "chi_square_df = pd.DataFrame({\n",
        "    'Feature' : tfidf_vectorizer.get_feature_names_out(),\n",
        "    'Chi_square value': chi_square_score\n",
        "}).sort_values(by='Chi_square value', ascending=False)\n",
        "\n",
        "#Displaying the chi square test results\n",
        "print(\"Chi square results\\n\",chi_square_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "outputId": "0a8d0f13-acaa-4fa5-8ee7-ed5d2df32e57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Reviews Based on Similarity to the Query in descending order:\n",
            "\n",
            "   Rank                                             Review  Similarity\n",
            "0     1        I loved this movie! The actors were amazing    0.819950\n",
            "1     2                         Just Superb. Amazing Story    0.728510\n",
            "2     3  Highly recommend! Amazing screenplay, directio...    0.727544\n",
            "3     4  The movie is a total flop. No story, no screen...    0.716364\n",
            "4     5     Not bad, the movie was okay for the first half    0.656546\n",
            "5     6  Its okay, the movie did not reach my expectations    0.653633\n",
            "6     7      Dont skip this movie! Everyone must watch it.    0.641844\n",
            "7     8       Just wasted three hours of my valuable time!    0.601925\n",
            "8     9     Dont watch this movie. Its a total time waste!    0.596875\n",
            "9    10       This was the worst purchase I have ever made    0.549333\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "#Importing libraries\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import BertTokenizer, TFBertModel, logging\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "#Loading BERT Model and tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#Defining function to generate BERT embeddings\n",
        "\n",
        "def bert_embedding(text):\n",
        "    input = bert_tokenizer(text, return_tensors='tf', truncation= True, padding=True)\n",
        "    output = bert_model(**input)\n",
        "    return tf.reduce_mean(output.last_hidden_state, axis=1)\n",
        "\n",
        "#Generating embeddings for all reviews present\n",
        "embedding_of_review = tf.concat([bert_embedding(review) for review in df['review']], axis=0)\n",
        "\n",
        "#Defining a query to match relevant documents\n",
        "query = \"Great movie with amazing acting!\"\n",
        "embedding_of_query = bert_embedding(query)\n",
        "\n",
        "#Calculating cosine similarity between query and each review\n",
        "similarity = cosine_similarity(embedding_of_query.numpy(), embedding_of_review.numpy())\n",
        "\n",
        "# Ranking the reviews based on similarities\n",
        "sorting_similarity = similarity.argsort()[0][::-1]\n",
        "reviews_ranking = [df['review'].iloc[i] for i in sorting_similarity]\n",
        "similarity_ranking = [similarity[0][i] for i in sorting_similarity ]\n",
        "\n",
        "#Displaying the reviews with their similarity values\n",
        "dataset = pd.DataFrame({'Rank' : range(1, len(reviews_ranking) + 1), 'Review' : reviews_ranking, 'Similarity' : similarity_ranking})\n",
        "\n",
        "\n",
        "#Displaying the ranking in descending order\n",
        "print(\"\\nReviews Based on Similarity to the Query in descending order:\\n\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEs-OoDEhTW4"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUKC7suYhVl0"
      },
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "outputId": "4adfcc45-12a6-4ff3-bded-5012ddd0dd45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\nLearning Experience: While doing this task of extracting features from text data, I found it quite interesting and helpful in grasping the fundamentals of NLP. I learned one such important aspect, use of TF-IDF and N-grams for feature extraction and how they helped in placing a value for a word pertinent to the context of the document and its size. These techniques helped to gain insight in how importance of certain words and phrases could be measured in relation to the entire document and its relative silence. This was an entirely new concept as I only came to learn how to use BERT embeddings which are highly efficient in obtaining contextual vectors of texts. This exercise helped emphasize the need for cleaning and transforming text before it is processed by machine learning algorithms, thus improving both the practical and theoretical aspects of my knowledge in NLP.\\n\\nChallenges Encountered: Regarding this exercise, I can say that the process has not been smooth since I had to encounter some challenges. Firstly, there were challenges on loading the BERT model especially in respect to the many warnings on model weight initialization. It also required time to learn what those warnings were, as well as how to disable them. Further, the task was made difficult by the need to calculate cosine similarity as well as using TensorFlow tensors properly. It also required a level of precision that was necessary to ensure that every codec component was suitable and that the final output was well structured in a table.\\n\\nRelevance to Your Field of Study: This exercise is very significant in relation to my study. Any document analysis procedure which features text classification included in it has discourse feature extraction as one of its vital processes. Wise feature extraction from textual information makes one capable of training a more efficient model and interpreting the outcome in a more relevant context. Also, it is an indication of the reliance towards using readily available models on different NLP tasks, which is crucial in the field of NLP when working with advanced models such as BERT. This exercise has further made me more eager to engage in more advanced NLP tasks such as text analytics and classification which are very important in real life scenarios.\\n\\n\\n'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "Learning Experience: While doing this task of extracting features from text data, I found it quite interesting and helpful in grasping the fundamentals of NLP. I learned one such important aspect, use of TF-IDF and N-grams for feature extraction and how they helped in placing a value for a word pertinent to the context of the document and its size. These techniques helped to gain insight in how importance of certain words and phrases could be measured in relation to the entire document and its relative silence. This was an entirely new concept as I only came to learn how to use BERT embeddings which are highly efficient in obtaining contextual vectors of texts. This exercise helped emphasize the need for cleaning and transforming text before it is processed by machine learning algorithms, thus improving both the practical and theoretical aspects of my knowledge in NLP.\n",
        "\n",
        "Challenges Encountered: Regarding this exercise, I can say that the process has not been smooth since I had to encounter some challenges. Firstly, there were challenges on loading the BERT model especially in respect to the many warnings on model weight initialization. It also required time to learn what those warnings were, as well as how to disable them. Further, the task was made difficult by the need to calculate cosine similarity as well as using TensorFlow tensors properly. It also required a level of precision that was necessary to ensure that every codec component was suitable and that the final output was well structured in a table.\n",
        "\n",
        "Relevance to Your Field of Study: This exercise is very significant in relation to my study. Any document analysis procedure which features text classification included in it has discourse feature extraction as one of its vital processes. Wise feature extraction from textual information makes one capable of training a more efficient model and interpreting the outcome in a more relevant context. Also, it is an indication of the reliance towards using readily available models on different NLP tasks, which is crucial in the field of NLP when working with advanced models such as BERT. This exercise has further made me more eager to engage in more advanced NLP tasks such as text analytics and classification which are very important in real life scenarios.\n",
        "\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
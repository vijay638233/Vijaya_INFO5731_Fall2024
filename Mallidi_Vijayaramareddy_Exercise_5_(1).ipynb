{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijay638233/Vijaya_INFO5731_Fall2024/blob/main/Mallidi_Vijayaramareddy_Exercise_5_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU-pLW33lpcS"
      },
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loi8Sh7UE6ha"
      },
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "outputId": "b0ac03cd-6b3d-427e-a4c4-b47c9db36ff0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package punkt to /Users/venya/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import ssl\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "nltk.download('punkt')\n",
        "train_file = \"stsa-train.txt\"\n",
        "test_file = \"stsa-test.txt\"\n",
        "\n",
        "def load_data(filepath):\n",
        "    data = []\n",
        "    labels = []\n",
        "    with open(filepath, 'r') as file:\n",
        "        for line in file:\n",
        "            label, text = line.split(maxsplit=1)\n",
        "            data.append(text.strip())\n",
        "            labels.append(int(label))\n",
        "    return pd.DataFrame({'text': data, 'label': labels})\n",
        "\n",
        "train_data = load_data(train_file)\n",
        "test_data = load_data(test_file)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    train_data['text'], train_data['label'], test_size=0.2, random_state=42\n",
        ")\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_val_tfidf = tfidf.transform(X_val)\n",
        "X_test_tfidf = tfidf.transform(test_data['text'])\n",
        "\n",
        "def train_word2vec(sentences):\n",
        "    tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "    model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
        "    return model\n",
        "\n",
        "def get_sentence_embedding(model, sentence):\n",
        "    words = word_tokenize(sentence.lower())\n",
        "    embeddings = [model.wv[word] for word in words if word in model.wv]\n",
        "    if len(embeddings) > 0:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "word2vec_model = train_word2vec(X_train)\n",
        "X_train_w2v = np.array([get_sentence_embedding(word2vec_model, sentence) for sentence in X_train])\n",
        "X_val_w2v = np.array([get_sentence_embedding(word2vec_model, sentence) for sentence in X_val])\n",
        "X_test_w2v = np.array([get_sentence_embedding(word2vec_model, sentence) for sentence in test_data['text']])\n",
        "\n",
        "def get_bert_embeddings(sentences, tokenizer, model):\n",
        "    tokenized = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokenized)\n",
        "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "X_train_bert = get_bert_embeddings(X_train.tolist(), bert_tokenizer, bert_model)\n",
        "X_val_bert = get_bert_embeddings(X_val.tolist(), bert_tokenizer, bert_model)\n",
        "X_test_bert = get_bert_embeddings(test_data['text'].tolist(), bert_tokenizer, bert_model)\n",
        "models = {\n",
        "    \"MultinomialNB\": MultinomialNB(),\n",
        "    \"SVM\": SVC(kernel='linear', probability=True),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(),\n",
        "    \"RandomForest\": RandomForestClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "def evaluate_models(models, X_train, y_train, X_val, y_val):\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        # Perform 10-fold cross-validation\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy')\n",
        "        print(f\"{name} - Mean CV Accuracy: {np.mean(scores):.4f}\")\n",
        "\n",
        "        # Train on full training data\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Validate\n",
        "        y_val_pred = model.predict(X_val)\n",
        "        results[name] = {\n",
        "            \"Accuracy\": accuracy_score(y_val, y_val_pred),\n",
        "            \"Precision\": precision_score(y_val, y_val_pred),\n",
        "            \"Recall\": recall_score(y_val, y_val_pred),\n",
        "            \"F1 Score\": f1_score(y_val, y_val_pred)\n",
        "        }\n",
        "    return results\n",
        "print(\"\\nTF-IDF Results:\")\n",
        "tfidf_results = evaluate_models(models, X_train_tfidf, y_train, X_val_tfidf, y_val)\n",
        "\n",
        "print(\"\\nWord2Vec Results:\")\n",
        "word2vec_results = evaluate_models(models, X_train_w2v, y_train, X_val_w2v, y_val)\n",
        "\n",
        "print(\"\\nBERT Results:\")\n",
        "bert_results = evaluate_models(models, X_train_bert, y_train, X_val_bert, y_val)\n",
        "best_model = models[\"RandomForest\"]\n",
        "best_model.fit(X_train_tfidf, y_train)\n",
        "y_test_pred = best_model.predict(X_test_tfidf)\n",
        "test_accuracy = accuracy_score(test_data['label'], y_test_pred)\n",
        "print(f\"\\nTest Accuracy of Best Model: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "outputId": "e2c20ecb-92fd-4a64-e539-2c68dab23d90"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package punkt to /Users/venya/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                        Product Name Brand Name   Price  \\\n",
            "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "\n",
            "   Rating                                            Reviews  Review Votes  \n",
            "0       5  I feel so LUCKY to have found this used (phone...           1.0  \n",
            "1       4  nice phone, nice up grade from my pantach revu...           0.0  \n",
            "2       5                                       Very pleased           0.0  \n",
            "3       4  It works good but it goes slow sometimes but i...           0.0  \n",
            "4       4  Great phone to replace my lost phone. The only...           0.0  \n",
            "TF-IDF Matrix Shape:  (413770, 1000)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             Reviews  kmeans_cluster\n",
            "0  I feel so LUCKY to have found this used (phone...               2\n",
            "1  nice phone, nice up grade from my pantach revu...               2\n",
            "2                                       Very pleased               2\n",
            "3  It works good but it goes slow sometimes but i...               1\n",
            "4  Great phone to replace my lost phone. The only...               2\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# Ensure the NLTK punkt tokenizer is downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "df = pd.read_csv('Amazon_Unlocked_Mobile.csv')\n",
        "print(df.head())\n",
        "df.dropna(subset=['Reviews'], inplace=True)\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "X = vectorizer.fit_transform(df['Reviews'])\n",
        "\n",
        "print(\"TF-IDF Matrix Shape: \", X.shape)\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "df['kmeans_cluster'] = kmeans.fit_predict(X)\n",
        "print(df[['Reviews', 'kmeans_cluster']].head())\n",
        "\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5, metric='cosine')\n",
        "df['dbscan_cluster'] = dbscan.fit_predict(X)\n",
        "print(df[['Reviews', 'dbscan_cluster']].head())\n",
        "\n",
        "hierarchical = AgglomerativeClustering(n_clusters=5, affinity='cosine', linkage='average')\n",
        "df['hierarchical_cluster'] = hierarchical.fit_predict(X.toarray())\n",
        "print(df[['Reviews', 'hierarchical_cluster']].head())\n",
        "\n",
        "tokenized_reviews = df['Reviews'].apply(word_tokenize)\n",
        "model = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, min_count=1, workers=4)\n",
        "def get_review_vector(review):\n",
        "    vectors = [model.wv[word] for word in review if word in model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
        "\n",
        "df['word2vec_vector'] = tokenized_reviews.apply(get_review_vector)\n",
        "word2vec_vectors = np.vstack(df['word2vec_vector'].values)\n",
        "kmeans_word2vec = KMeans(n_clusters=5, random_state=42)\n",
        "df['word2vec_cluster'] = kmeans_word2vec.fit_predict(word2vec_vectors)\n",
        "print(df[['Reviews', 'word2vec_cluster']].head())\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "def get_bert_embeddings(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "df['bert_embedding'] = df['Reviews'].apply(get_bert_embeddings)\n",
        "bert_embeddings = np.vstack(df['bert_embedding'].values)\n",
        "kmeans_bert = KMeans(n_clusters=5, random_state=42)\n",
        "df['bert_cluster'] = kmeans_bert.fit_predict(bert_embeddings)\n",
        "print(df[['Reviews', 'bert_cluster']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRijW2aLGONl"
      },
      "source": [
        "**In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIYCj5qyGfSL"
      },
      "source": [
        "**Write your response here:**\n",
        "\n",
        "The clustering with K-means, DBSCAN, and Hierarchical clustering methods to the reviews yields different advantages and drawbacks. For example, K-means is a good cluster when the centroid structure exists since the model separates similar reviews through high-dimensional TF-IDF vector representations. Yet, it has difficulty clustering irregularly shaped clusters or noise regions. In application of dense clustering methods, this feature is advantageous when background noise needs to be separated from findings that are clusters of arbitrary shape. However, quite often than not, its hyperparameters settings like epsilon and min_samples hinder its effectiveness during real world applications especially with large datasets. Hierarchical clustering gives a systematic dendrogram of the interacted clusters, but this is however limited to smaller datasets as otherwise, the computational expense can be very high. Similarly, Word2Vec and BERT that work with semantic embeddings allow these clustering timed algorithms to group the reviews which have deeper, possibly latent similarities. Word2Vec works relatively well when only encompassing word-level semantics that are adequate for simpler sentences, in contrast to more advanced language context, sentence-level embeddings such as BERT that better approximates deeper lexico-semantics allowing for more accurate clustering when semantic comprehension is significant. Thus more enriched embeddings will help to considerably elevate the overall quality of the clustering especially in context-rich text data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEs-OoDEhTW4"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUKC7suYhVl0"
      },
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "This exercise presented an excellent opportunity to engage with and apply different algorithms for machine learning based text classification, thereby acquiring practical skills in applications such as sentiment analysis. It was also interesting to deal with such models as a Naive Bayes, SVM, Random Forest and consider how well they work in practice. Moreover, the application of state of the art techniques such as Word2Vec and BERT embeddings enhanced the learning experience through exposing the details of modern natural language processing (NLP) techniques.\n",
        "\n",
        "One of the major issues for some of the authors was the effective and efficient use of computational resources particularly when employing large embedding models such as BERT which posed challenges in terms of optimizations and memory management. At the other extreme, simpler strategies such as TF-IDF and MultinomialNB clearly illustrated how traditional approaches have some reasonable efficacy with at most elementary complexity. This equilibrium underscored the power plays between less but understandable models and most advanced but highly accurate approaches.\n",
        "\n",
        "With the application of a 10-fold cross-validation, the reliability of model evaluation and comparison was again underlined as performance indices, though model specific, were not overly reliant on particular divisions of the data set. This assisted in appreciating the extent to which model performance differs across folds.\n",
        "\n",
        "In conclusion, this project was a perfect blend of theory and practice. It deepened my appreciation of concepts such as text pre-processing, the feature engineering, and the evaluation of the machine learning model while giving an insight into how machine learning techniques such as in this case sentiment analysis can be used to solve real life problems. It was a difficult but enjoyable task, illustrating the wide range of techniques that can be applied in the modern day machine learning.\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}